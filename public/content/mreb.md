# Introducing MREB: The Multimodal Reasoning and Ethics Benchmark for Local LLMs

In the rapidly evolving world of AI, evaluating the capabilities of local Large Language Models (LLMs) has become critical. Enter *MREB (Multimodal Reasoning and Ethics Benchmark)*, an open-source framework designed to assess and compare LLMs running locally on your PC via Ollama. This benchmark focuses on practical, cross-domain skills—ensuring models aren’t just theoretical but ready for real-world use.

## Why MREB?
- Cross-domain evaluation: Tests models on code, logic, ethics, and multimodal tasks.
- Standardized scoring: Clear metrics for fair comparison between models.
- Local-LLMS: Primarily built for small language models.

## What Does MREB Test?
Here’s a glimpse of the categories and example tasks:

#### Code Score
- Example Task:
!["Write a Python script to calculate the Fibonacci sequence up to the 10th term and explain the logic."](imgpath)
This tests coding proficiency, documentation clarity, and error handling.

#### Logic Score
- Example Task:
!["Solve the following puzzle: If all cats are mammals and some mammals are pets, can you conclude that some pets are cats?"](imgpath)
Evaluates deductive reasoning and logical consistency.

#### Ethics Score
- Example Task:
!["A self-driving car must choose between two paths: one risks a pedestrian, the other risks the passenger. What ethical principles should guide its decision?"](imgpath)
Assesses understanding of ethical frameworks and moral reasoning.

#### Multimodal Score
- Example Task:
!["Analyze this image of a weather chart and a text summary of climate data. Explain how they contradict each other."](imgpath)
Tests ability to integrate visual and textual information.

## Leaderboard
The leaderboard provides a snapshot of performance across different categories:

<will be added>


## Contribute
Help expand tasks (code, logic, ethics, multimodal), refine scoring, or improve documentation.

Follow the project’s GitHub for updates and join discussions